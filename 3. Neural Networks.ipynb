{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module): #nn.Module 모든 신경망 모듈의 기본이 되는 클래스\n",
    "                      #각 층과 함수 등 신경망의 구성요소를 이 클래스 안에서 정의\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() #부모 클래스 상속\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution(=filter)\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3) #1 channel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3) #입력 채널 수, 출력 채널 수, 필터크기, (stride=1, padding=0)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16*6*6, 120)  # 6*6 from image dimension\n",
    "        # 입력 샘플 크기, 출력 샘플 크기\n",
    "        #16*6*6 : conv2 레이어를 지난 출력 채널의 수=16, 입력 이미지의 dimension=6*6\n",
    "        #nn.Linear함수 : 선형 함수를 사용해 입력으로부터 출력 계산, 내부 tensor에 가중치와 편향 저장\n",
    "        self.fc2 = nn.Linear(120, 84) #fc1의 출력 -> fc2의 입력\n",
    "        self.fc3 = nn.Linear(84, 10) #fc2의 출력 -> fc3의 출력\n",
    "\n",
    "    #x를 매개변수로 forward 함수를 정의\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) #conv1 계산 -> max pooling\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) #conv2 계산 -> max pooling\n",
    "        x = x.view(-1, self.num_flat_features(x)) # '.view()': tensor의 크기와 모양을 변경\n",
    "                                                  #num_flat_features(self,x) 함수 하단에 정의\n",
    "        x = F.relu(self.fc1(x)) #fc1 레이어 -> relu 계산(activation func)\n",
    "        #활성화 함수 : node에 들어오는 값들에 대해 비선형 함수를 통과시킨 후 다음 레이어로 전달\n",
    "        #활성화 함수 = 비선형 함수\n",
    "        #선형 함수인 h(x)=cx 를 활성화 함수로 사용한 3층 네트워크를 생각해보자\n",
    "        #y(x)=h(h(h(x)))로 결국 y(x)=ax와 똑같은 식 -> 은닉층이 없는 네트워크\n",
    "        #NN에서 층을 쌓고 싶다면 비선형 함수를 이용\n",
    "        x = F.relu(self.fc2(x)) #fc2 레이어 -> relu 계산\n",
    "        x = self.fc3(x) #최종 레이어이므로 활성화 함수 사용x\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s #size라는 목록의 각각의 항목 s에 대하여 계산\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net() #임의로 실행\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forward` 함수만 정의하고 나면, `backward`함수는 `autograd`를 사용해 자동으로 정의\n",
    "\n",
    "모델의 학습 가능한 매개변수들(=가중치)은 `net.parameters()`에 의해 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params=list(net.parameters()) #매개변수 리스트 생성\n",
    "print(len(params))\n",
    "print(params[0].size()) #conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0461, -0.0539, -0.0117, -0.1220, -0.0539,  0.0183, -0.0419,  0.0354,\n",
      "          0.0384, -0.0164]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input=torch.randn(1,1,32,32)\n",
    "out=net(input)\n",
    "print(out) #최종 fc3 outpt sample size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad() #모든 매개변수의 변화도 버퍼(gradient buffer)를 0으로 설정\n",
    "out.backward(torch.randn(1,10)) #무작위 값으로 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `torch.nn`은 미니배치만 지원, 샘플들의 미니배치만을 입력으로 받음\n",
    "#### `nnConv2D` 는 nSamples x nChannels x Height x Width\n",
    "#### 만약 하나의 샘플만 있다면, `input.unsqueeze(0)` 을 사용해 가짜 차원을 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> 요약\n",
    "\n",
    "`torch.Tensor` - `backward()` 같은 autograd 연산을 지원하는 다차원 배열\n",
    "\n",
    "또한 tensor에 대한 변화도(gradient)를 가짐\n",
    "\n",
    "`nn.Module` - 신경망 모듈. \n",
    "\n",
    "매개변수를 캡슐화 (gpu로 이동, 내보내기, 불러오기 등의 작업을 위한 헬퍼를 제공)\n",
    "\n",
    "`nn.Parameter` - tensor의 한 종류로, Module에 속성으로 할당될 때 자동으로 매개변수 등록\n",
    "\n",
    "`autograd.Function` - autograd 연산의 전방향과 역방향 정의를 구현\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 손실함수 (loss function) \n",
    " : 한 쌍의 입력으로 받아 출력이 target으로부터 얼마나 멀리 떨어져 있는지 추정하는 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9435, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#평균제곱오차 - nn.MSEloss\n",
    "output=net(input)\n",
    "target=torch.randn(10) #a dummy target, for ex\n",
    "target=target.view(1,-1) #make it the same shape as output\n",
    "# -1 은 차원 수 보고 알맞게 알아서 설정하라는 의미\n",
    "criterion=nn.MSELoss()\n",
    "\n",
    "loss=criterion(output,target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss.backward()`를 실행할 때, 전체 그래프는 손실에 대하여 미분되며,\n",
    "\n",
    "그래프 내의 `requires_Grad=True` 인 모든 tensor는 gradient가 누적된 `.grad` tensor를 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x000002493E3F53D0>\n",
      "<AddmmBackward object at 0x000002493E3F5EB0>\n",
      "<AccumulateGrad object at 0x000002493E3F53D0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn) #MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0]) #linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) #ReLU\n",
    "#각 단계의 gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 역전파 (backprop)\n",
    "오차를 역전파하기 위해서는 `loss.backward()` 만 해주면 됨\n",
    "\n",
    "변화도가 기존의 것에 누적될 수 있기 때문에 기존 변화도를 없애는 작업 필요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0135, -0.0126,  0.0012,  0.0125,  0.0189, -0.0043])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad() #zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 가중치 갱신\n",
    "많이 사용되는 단순한 갱신 규칙 : 확률적 경사하강법 SGD\n",
    "\n",
    "가중치=가중치-학습율*변화도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data*learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 구성 시 다양한 갱신 규칙을 사용하기 위해 `torch.optim` 에 구현되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#optimizer를 생성\n",
    "optimizer=optim.SGD(net.parameters(),lr=0.01)\n",
    "#SGD를 직접 정의하지 않고, torch.optim 패키지 속 모듈 사용\n",
    "\n",
    "#학습과정(training loop) :\n",
    "optimizer.zero_grad() #zero the gradient buffers\n",
    "output=net(input)\n",
    "loss=criterion(output,target)\n",
    "loss.backward()\n",
    "optimizer.step() #does the update\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
